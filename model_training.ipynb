{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8482235d",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- Make sure that when splitting dataset it accounts for batching or not\n",
    "\n",
    "- Reduce shuffle and see reason for performance problems\n",
    "\n",
    "- Baysian hyper paramater optimization\n",
    "\n",
    "- Find way tensorflow is under utilizing gpu with tensorflow\n",
    "    - Use tensorbaord\n",
    "    \n",
    "- Create seperate validation and train file sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184010c1",
   "metadata": {
    "id": "uPojRlNQPJub",
    "papermill": {
     "duration": 0.03195,
     "end_time": "2022-03-19T19:43:05.365643",
     "exception": false,
     "start_time": "2022-03-19T19:43:05.333693",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c302be40",
   "metadata": {
    "id": "oOtnY2y8Om-1",
    "papermill": {
     "duration": 0.028497,
     "end_time": "2022-03-19T19:43:05.424262",
     "exception": false,
     "start_time": "2022-03-19T19:43:05.395765",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d33c47f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-19T19:43:05.493206Z",
     "iopub.status.busy": "2022-03-19T19:43:05.483222Z",
     "iopub.status.idle": "2022-03-19T19:43:10.431279Z",
     "shell.execute_reply": "2022-03-19T19:43:10.430528Z",
     "shell.execute_reply.started": "2022-03-19T18:27:05.007611Z"
    },
    "id": "1Mf4kbp8yOxd",
    "papermill": {
     "duration": 4.979471,
     "end_time": "2022-03-19T19:43:10.431438",
     "exception": false,
     "start_time": "2022-03-19T19:43:05.451967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Dict, List, Optional, Text, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import datetime\n",
    "import timeit\n",
    "import os\n",
    "import random\n",
    "from keras import backend as K\n",
    "\n",
    "#### For testing purposes\n",
    "random.seed(1)\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "594cd683",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_GPU_THREAD_MODE'] = 'gpu_private'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441814b1",
   "metadata": {
    "id": "aVb2hhOcgVwU",
    "papermill": {
     "duration": 0.02668,
     "end_time": "2022-03-19T19:43:10.485754",
     "exception": false,
     "start_time": "2022-03-19T19:43:10.459074",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6b2774",
   "metadata": {
    "id": "awWJ00JeOyuO",
    "papermill": {
     "duration": 0.026626,
     "end_time": "2022-03-19T19:43:10.539675",
     "exception": false,
     "start_time": "2022-03-19T19:43:10.513049",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Enter the Unix glob file pattern of the data files.\n",
    "\n",
    "Here we load the training data. All the data are stored in TensorFlow Record files.\n",
    "Replace 'train' with 'eval' or 'test' to load the evaluation or testing data, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1cd89e",
   "metadata": {
    "id": "OabUJMGqO9NE",
    "papermill": {
     "duration": 0.026956,
     "end_time": "2022-03-19T19:43:10.655917",
     "exception": false,
     "start_time": "2022-03-19T19:43:10.628961",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Run the following three cells to define the required library functions for loading the data.\n",
    "\n",
    "The first cell defines the name of the variables in the input files and the corrresponding data statistics. The statistics can be used for preprocessing the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae79c7c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-19T19:43:10.713719Z",
     "iopub.status.busy": "2022-03-19T19:43:10.712941Z",
     "iopub.status.idle": "2022-03-19T19:43:10.723815Z",
     "shell.execute_reply": "2022-03-19T19:43:10.724638Z",
     "shell.execute_reply.started": "2022-03-19T18:27:09.617343Z"
    },
    "id": "GTTV3tjjCcdn",
    "papermill": {
     "duration": 0.04193,
     "end_time": "2022-03-19T19:43:10.724862",
     "exception": false,
     "start_time": "2022-03-19T19:43:10.682932",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Constants for the data reader.\"\"\"\n",
    "\n",
    "INPUT_FEATURES = ['elevation', 'th', 'vs',  'tmmn', 'tmmx', 'sph', \n",
    "                  'pr', 'pdsi', 'NDVI', 'population', 'erc', 'PrevFireMask']\n",
    "\n",
    "\n",
    "OUTPUT_FEATURES = ['detectionImage']\n",
    "\n",
    "# Data statistics \n",
    "# For each variable, the statistics are ordered in the form:\n",
    "# (min_clip, max_clip, mean, standard deviation)\n",
    "DATA_STATS = {\n",
    "    # Elevation in m.\n",
    "    # 0.1 percentile, 99.9 percentile\n",
    "    'elevation': (0.0, 3141.0, 657.3003, 649.0147),\n",
    "    \n",
    "    # Drought Index (Palmer Drought Severity Index)\n",
    "    # 0.1 percentile, 99.9 percentile\n",
    "    'pdsi': (-6.12974870967865, 7.876040384292651, -0.0052714925, 2.6823447),\n",
    "    \n",
    "    #Vegetation index (times 10,000 maybe, since it's supposed to be b/w -1 and 1?)\n",
    "    'NDVI': (-9821.0, 9996.0, 5157.625, 2466.6677),  # min, max\n",
    "   \n",
    "    # Precipitation in mm.\n",
    "    # Negative values do not make sense, so min is set to 0.\n",
    "    # 0., 99.9 percentile\n",
    "    'pr': (0.0, 44.53038024902344, 1.7398051, 4.482833),\n",
    "   \n",
    "    # Specific humidity.\n",
    "    # Negative values do not make sense, so min is set to 0.\n",
    "    # The range of specific humidity is up to 100% so max is 1.\n",
    "    'sph': (0., 1., 0.0071658953, 0.0042835088),\n",
    "    \n",
    "    # Wind direction in degrees clockwise from north.\n",
    "    # Thus min set to 0 and max set to 360.\n",
    "    'th': (0., 360.0, 190.32976, 72.59854),\n",
    "    \n",
    "    # Min/max temperature in Kelvin.\n",
    "    \n",
    "    #Min temp\n",
    "    # -20 degree C, 99.9 percentile\n",
    "    'tmmn': (253.15, 298.94891357421875, 281.08768, 8.982386),\n",
    "    \n",
    "    #Max temp\n",
    "    # -20 degree C, 99.9 percentile\n",
    "    'tmmx': (253.15, 315.09228515625, 295.17383, 9.815496),\n",
    "    \n",
    "    # Wind speed in m/s.\n",
    "    # Negative values do not make sense, given there is a wind direction.\n",
    "    # 0., 99.9 percentile\n",
    "    'vs': (0.0, 10.024310074806237, 3.8500874, 1.4109988),\n",
    "    \n",
    "    # NFDRS fire danger index energy release component expressed in BTU's per\n",
    "    # square foot.\n",
    "    # Negative values do not make sense. Thus min set to zero.\n",
    "    # 0., 99.9 percentile\n",
    "    'erc': (0.0, 106.24891662597656, 37.326267, 20.846027),\n",
    "    \n",
    "    # Population density\n",
    "    # min, 99.9 percentile\n",
    "    'population': (0., 2534.06298828125, 25.531384, 154.72331),\n",
    "    \n",
    "    # We don't want to normalize the FireMasks.\n",
    "    # 1 indicates fire, 0 no fire, -1 unlabeled data\n",
    "    'PrevFireMask': (0., 1., 0., 1.),\n",
    "    'detectionImage': (0, 1., 0., 1.)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226b5f94",
   "metadata": {
    "papermill": {
     "duration": 0.027365,
     "end_time": "2022-03-19T19:43:10.779396",
     "exception": false,
     "start_time": "2022-03-19T19:43:10.752031",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The following cell defines cropping functions for extracting regions of the desired size from the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9e120e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-19T19:43:10.837327Z",
     "iopub.status.busy": "2022-03-19T19:43:10.836642Z",
     "iopub.status.idle": "2022-03-19T19:43:10.846573Z",
     "shell.execute_reply": "2022-03-19T19:43:10.845964Z",
     "shell.execute_reply.started": "2022-03-19T18:27:09.632855Z"
    },
    "id": "QqGYv21hD-2q",
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.039897,
     "end_time": "2022-03-19T19:43:10.846723",
     "exception": false,
     "start_time": "2022-03-19T19:43:10.806826",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Library of common functions used in deep learning neural networks.\n",
    "\"\"\"\n",
    "#YOU PROBABLY WILL NOT USE THESE.\n",
    "\n",
    "def random_crop_input_and_output_images(\n",
    "        input_img: tf.Tensor,\n",
    "        output_img: tf.Tensor,\n",
    "        sample_size: int,\n",
    "        num_in_channels: int,\n",
    "        num_out_channels: int,\n",
    ") -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "    \"\"\"Randomly axis-align crop input and output image tensors.\n",
    "\n",
    "    Args:\n",
    "        input_img: tensor with dimensions HWC.\n",
    "        output_img: tensor with dimensions HWC.\n",
    "        sample_size: side length (square) to crop to.\n",
    "        num_in_channels: number of channels in input_img.\n",
    "        num_out_channels: number of channels in output_img.\n",
    "    Returns:\n",
    "        input_img: tensor with dimensions HWC.\n",
    "        output_img: tensor with dimensions HWC.\n",
    "    \"\"\"\n",
    "    combined = tf.concat([input_img, output_img], axis=2)\n",
    "    combined = tf.image.random_crop(\n",
    "            combined,\n",
    "            [sample_size, sample_size, num_in_channels + num_out_channels])\n",
    "    input_img = combined[:, :, 0:num_in_channels]\n",
    "    output_img = combined[:, :, -num_out_channels:]\n",
    "    return input_img, output_img\n",
    "\n",
    "\n",
    "def center_crop_input_and_output_images(\n",
    "        input_img: tf.Tensor,\n",
    "        output_img: tf.Tensor,\n",
    "        sample_size: int,\n",
    ") -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "    \"\"\"Center crops input and output image tensors.\n",
    "\n",
    "    Args:\n",
    "        input_img: tensor with dimensions HWC.\n",
    "        output_img: tensor with dimensions HWC.\n",
    "        sample_size: side length (square) to crop to.\n",
    "    Returns:\n",
    "        input_img: tensor with dimensions HWC.\n",
    "        output_img: tensor with dimensions HWC.\n",
    "    \"\"\"\n",
    "    central_fraction = sample_size / input_img.shape[0]\n",
    "    input_img = tf.image.central_crop(input_img, central_fraction)\n",
    "    output_img = tf.image.central_crop(output_img, central_fraction)\n",
    "    return input_img, output_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40090830",
   "metadata": {
    "papermill": {
     "duration": 0.026961,
     "end_time": "2022-03-19T19:43:10.900906",
     "exception": false,
     "start_time": "2022-03-19T19:43:10.873945",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The following cell provides code for parsing the contents of the TensorFlow Record files. In addition to loading the data, it also offers functions for various preprocessing operations, such as clipping, rescaling, or normalizing the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "21c4adf7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-19T19:43:10.987604Z",
     "iopub.status.busy": "2022-03-19T19:43:10.986767Z",
     "iopub.status.idle": "2022-03-19T19:43:10.989997Z",
     "shell.execute_reply": "2022-03-19T19:43:10.989315Z",
     "shell.execute_reply.started": "2022-03-19T18:27:09.650015Z"
    },
    "id": "VBvI9FuGEC09",
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.062131,
     "end_time": "2022-03-19T19:43:10.990138",
     "exception": false,
     "start_time": "2022-03-19T19:43:10.928007",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Dataset reader for Earth Engine data.\"\"\"\n",
    "\n",
    "def _get_base_key(key: Text) -> Text:\n",
    "    \"\"\"Extracts the base key from the provided key.\n",
    "\n",
    "    Earth Engine exports TFRecords containing each data variable with its\n",
    "    corresponding variable name. In the case of time sequences, the name of the\n",
    "    data variable is of the form 'variable_1', 'variable_2', ..., 'variable_n',\n",
    "    where 'variable' is the name of the variable, and n the number of elements\n",
    "    in the time sequence. Extracting the base key ensures that each step of the\n",
    "    time sequence goes through the same normalization steps.\n",
    "    The base key obeys the following naming pattern: '([a-zA-Z]+)'\n",
    "    For instance, for an input key 'variable_1', this function returns 'variable'.\n",
    "    For an input key 'variable', this function simply returns 'variable'.\n",
    "\n",
    "    Args:\n",
    "        key: Input key.\n",
    "\n",
    "    Returns:\n",
    "        The corresponding base key.\n",
    "\n",
    "    Raises:\n",
    "        ValueError when `key` does not match the expected pattern.\n",
    "    \"\"\"\n",
    "    match = re.match(r'([a-zA-Z]+)', key)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    raise ValueError(\n",
    "            'The provided key does not match the expected pattern: {}'.format(key))\n",
    "\n",
    "\n",
    "def _clip_and_rescale(inputs: tf.Tensor, key: Text) -> tf.Tensor:\n",
    "    \"\"\"Clips and rescales inputs with the stats corresponding to `key`.\n",
    "\n",
    "    Args:\n",
    "        inputs: Inputs to clip and rescale.\n",
    "        key: Key describing the inputs.\n",
    "\n",
    "    Returns:\n",
    "        Clipped and rescaled input.\n",
    "\n",
    "    Raises:\n",
    "        ValueError if there are no data statistics available for `key`.\n",
    "    \"\"\"\n",
    "    base_key = _get_base_key(key)\n",
    "    if base_key not in DATA_STATS:\n",
    "        raise ValueError(\n",
    "                'No data statistics available for the requested key: {}.'.format(key))\n",
    "    min_val, max_val, _, _ = DATA_STATS[base_key]\n",
    "    inputs = tf.clip_by_value(inputs, min_val, max_val)\n",
    "    return tf.math.divide_no_nan((inputs - min_val), (max_val - min_val))\n",
    "\n",
    "\n",
    "def _clip_and_normalize(inputs: tf.Tensor, key: Text) -> tf.Tensor:\n",
    "    \"\"\"Clips and normalizes inputs with the stats corresponding to `key`.\n",
    "\n",
    "    Args:\n",
    "        inputs: Inputs to clip and normalize.\n",
    "        key: Key describing the inputs.\n",
    "\n",
    "    Returns:\n",
    "        Clipped and normalized input.\n",
    "\n",
    "    Raises:\n",
    "        ValueError if there are no data statistics available for `key`.\n",
    "    \"\"\"\n",
    "    base_key = _get_base_key(key)\n",
    "    if base_key not in DATA_STATS:\n",
    "        raise ValueError(\n",
    "                'No data statistics available for the requested key: {}.'.format(key))\n",
    "    min_val, max_val, mean, std = DATA_STATS[base_key]\n",
    "    inputs = tf.clip_by_value(inputs, min_val, max_val)\n",
    "    inputs = inputs - mean\n",
    "    return tf.math.divide_no_nan(inputs, std)\n",
    "\n",
    "def _get_features_dict(\n",
    "        sample_size: int,\n",
    "        features: List[Text],\n",
    ") -> Dict[Text, tf.io.FixedLenFeature]:\n",
    "    \"\"\"Creates a features dictionary for TensorFlow IO.\n",
    "\n",
    "    Args:\n",
    "        sample_size: Size of the input tiles (square).\n",
    "        features: List of feature names.\n",
    "\n",
    "    Returns:\n",
    "        A features dictionary for TensorFlow IO.\n",
    "    \"\"\"\n",
    "    sample_shape = [sample_size, sample_size]\n",
    "    features = set(features)\n",
    "    columns = [\n",
    "            tf.io.FixedLenFeature(shape=sample_shape, dtype=tf.float32)\n",
    "            for _ in features\n",
    "    ]\n",
    "    return dict(zip(features, columns))\n",
    "\n",
    "\n",
    "def _parse_fn(\n",
    "        example_proto: tf.train.Example, data_size: int, sample_size: int,\n",
    "        num_in_channels: int, clip_and_normalize: bool,\n",
    "        clip_and_rescale: bool, random_crop: bool, center_crop: bool,\n",
    ") -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "    \"\"\"Reads a serialized example.\n",
    "\n",
    "    Args:\n",
    "        example_proto: A TensorFlow example protobuf.\n",
    "        data_size: Size of tiles (square) as read from input files.\n",
    "        sample_size: Size the tiles (square) when input into the model.\n",
    "        num_in_channels: Number of input channels.\n",
    "        clip_and_normalize: True if the data should be clipped and normalized.\n",
    "        clip_and_rescale: True if the data should be clipped and rescaled.\n",
    "        random_crop: True if the data should be randomly cropped.\n",
    "        center_crop: True if the data should be cropped in the center.\n",
    "\n",
    "    Returns:\n",
    "        (input_img, output_img) tuple of inputs and outputs to the ML model.\n",
    "    \"\"\"\n",
    "    if (random_crop and center_crop):\n",
    "        raise ValueError('Cannot have both random_crop and center_crop be True')\n",
    "    input_features, output_features = INPUT_FEATURES, OUTPUT_FEATURES\n",
    "    feature_names = input_features + output_features\n",
    "    features_dict = _get_features_dict(data_size, feature_names)\n",
    "    features = tf.io.parse_single_example(example_proto, features_dict)\n",
    "\n",
    "    if clip_and_normalize:\n",
    "        inputs_list = [\n",
    "                _clip_and_normalize(features.get(key), key) for key in input_features\n",
    "        ]\n",
    "    elif clip_and_rescale:\n",
    "        inputs_list = [\n",
    "                _clip_and_rescale(features.get(key), key) for key in input_features\n",
    "        ]\n",
    "    else:\n",
    "        inputs_list = [features.get(key) for key in input_features]\n",
    "\n",
    "    inputs_stacked = tf.stack(inputs_list, axis=0)\n",
    "    input_img = tf.transpose(inputs_stacked, [1, 2, 0])\n",
    "\n",
    "    outputs_list = [features.get(key) for key in output_features]\n",
    "    assert outputs_list, 'outputs_list should not be empty'\n",
    "    outputs_stacked = tf.stack(outputs_list, axis=0)\n",
    "\n",
    "    outputs_stacked_shape = outputs_stacked.get_shape().as_list()\n",
    "    assert len(outputs_stacked.shape) == 3, ('outputs_stacked should be rank 3'\n",
    "                                            'but dimensions of outputs_stacked'\n",
    "                                            f' are {outputs_stacked_shape}')\n",
    "    output_img = tf.transpose(outputs_stacked, [1, 2, 0])\n",
    "\n",
    "    if random_crop:\n",
    "        input_img, output_img = random_crop_input_and_output_images(\n",
    "                input_img, output_img, sample_size, num_in_channels, 1)\n",
    "    if center_crop:\n",
    "        input_img, output_img = center_crop_input_and_output_images(\n",
    "                input_img, output_img, sample_size)\n",
    "    return input_img, output_img\n",
    "\n",
    "\n",
    "def get_dataset(fileNames: list[str], data_size: int, sample_size: int,\n",
    "                                batch_size: int, num_in_channels: int, compression_type: Text,\n",
    "                                clip_and_normalize: bool, clip_and_rescale: bool,\n",
    "                                random_crop: bool, center_crop: bool) -> tf.data.Dataset:\n",
    "    \"\"\"Gets the dataset from the file pattern.\n",
    "\n",
    "    Args:\n",
    "        file_pattern: Input file pattern.\n",
    "        data_size: Size of tiles (square) as read from input files.\n",
    "        sample_size: Size the tiles (square) when input into the model.\n",
    "        batch_size: Batch size.\n",
    "        num_in_channels: Number of input channels.\n",
    "        compression_type: Type of compression used for the input files.\n",
    "        clip_and_normalize: True if the data should be clipped and normalized, False\n",
    "            otherwise.\n",
    "        clip_and_rescale: True if the data should be clipped and rescaled, False\n",
    "            otherwise.\n",
    "        random_crop: True if the data should be randomly cropped.\n",
    "        center_crop: True if the data shoulde be cropped in the center.\n",
    "\n",
    "    Returns:\n",
    "        A TensorFlow dataset loaded from the input file pattern, with features\n",
    "        described in the constants, and with the shapes determined from the input\n",
    "        parameters to this function.\n",
    "    \"\"\"\n",
    "    options = tf.data.Options()\n",
    "    options.experimental_deterministic = False\n",
    "\n",
    "\n",
    "    if (clip_and_normalize and clip_and_rescale):\n",
    "        raise ValueError('Cannot have both normalize and rescale.')\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(fileNames)\n",
    "\n",
    "    dataset = dataset.interleave(\n",
    "            lambda x: tf.data.TFRecordDataset(x, compression_type=compression_type, num_parallel_reads=tf.data.AUTOTUNE),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)\n",
    "\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.map(\n",
    "            lambda x: _parse_fn(# pylint: disable=g-long-lambda\n",
    "                    x, data_size, sample_size, num_in_channels, clip_and_normalize,\n",
    "                    clip_and_rescale, random_crop, center_crop),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27b04e7",
   "metadata": {
    "id": "rdNDytnsPTVK",
    "papermill": {
     "duration": 0.026729,
     "end_time": "2022-03-19T19:43:11.044236",
     "exception": false,
     "start_time": "2022-03-19T19:43:11.017507",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Load the dataset.\n",
    "\n",
    "The data are stored as 64x64 km regions. For each data sample, we extract a random 32x32 km region. In the following function call, we do not clip, rescale or normalize the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3e2da326",
   "metadata": {},
   "outputs": [],
   "source": [
    "side_length = 64 #length of the side of the square you select (so, e.g. pick 64 if you don't want any random cropping)\n",
    "num_obs = 256 #batch size, higher batch size results in faster training and higher gpu utilization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "de2fc971",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainRatio = 0.8\n",
    "validRatio = 0.1\n",
    "testRatio = 0.1\n",
    "\n",
    "assert trainRatio + validRatio + testRatio <= 1, \"Datasets need to add up to 1 or less\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "914b2df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "allFiles = [os.path.join(\"./data/\", f) for f in os.listdir(\"./data/\") if os.path.isfile(os.path.join(\"./data/\", f))]\n",
    "random.shuffle(allFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "95b3dd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "numFiles = len(allFiles)\n",
    "\n",
    "trainSize = int(trainRatio * numFiles)\n",
    "validSize = int(validRatio * numFiles)\n",
    "testSize = int(testRatio * numFiles)\n",
    "\n",
    "train_files = allFiles[:trainSize]\n",
    "valid_files = allFiles[trainSize:trainSize+validSize]\n",
    "test_files = allFiles[trainSize+validSize:trainSize+validSize+testSize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a8c34fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = get_dataset(\n",
    "      train_files,\n",
    "      data_size=64,\n",
    "      sample_size=side_length,\n",
    "      batch_size=num_obs,\n",
    "      num_in_channels=12,\n",
    "      compression_type=None,\n",
    "      clip_and_normalize=False,\n",
    "      clip_and_rescale=True,\n",
    "      random_crop=True,\n",
    "      center_crop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cbba2542",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ds = get_dataset(\n",
    "      valid_files,\n",
    "      data_size=64,\n",
    "      sample_size=side_length,\n",
    "      batch_size=num_obs,\n",
    "      num_in_channels=12,\n",
    "      compression_type=None,\n",
    "      clip_and_normalize=False,\n",
    "      clip_and_rescale=True,\n",
    "      random_crop=True,\n",
    "      center_crop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20a78e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = get_dataset(\n",
    "      test_files,\n",
    "      data_size=64,\n",
    "      sample_size=side_length,\n",
    "      batch_size=num_obs,\n",
    "      num_in_channels=12,\n",
    "      compression_type=None,\n",
    "      clip_and_normalize=False,\n",
    "      clip_and_rescale=True,\n",
    "      random_crop=True,\n",
    "      center_crop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7919d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ds_size(dataset):\n",
    "    num_elements = 0\n",
    "    for element in dataset:\n",
    "        num_elements += 1\n",
    "    return num_elements\n",
    "\n",
    "print(f\"size of dataset is {find_ds_size(x)}, with batch size of {num_obs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbd1770",
   "metadata": {
    "id": "bzCgCoxtgP-f",
    "papermill": {
     "duration": 0.0562,
     "end_time": "2022-03-19T19:43:16.660951",
     "exception": false,
     "start_time": "2022-03-19T19:43:16.604751",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Plotting function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33f6419",
   "metadata": {
    "papermill": {
     "duration": 0.031646,
     "end_time": "2022-03-19T19:43:16.723648",
     "exception": false,
     "start_time": "2022-03-19T19:43:16.692002",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's plot the data!\n",
    "\n",
    "First we define the names for each of our variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a73cd70e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-19T19:43:11.876216Z",
     "iopub.status.busy": "2022-03-19T19:43:11.875461Z",
     "iopub.status.idle": "2022-03-19T19:43:12.148454Z",
     "shell.execute_reply": "2022-03-19T19:43:12.147798Z",
     "shell.execute_reply.started": "2022-03-19T18:29:27.964832Z"
    },
    "id": "Ml7Rg8aCQiTT",
    "papermill": {
     "duration": 0.306637,
     "end_time": "2022-03-19T19:43:12.148657",
     "exception": false,
     "start_time": "2022-03-19T19:43:11.842020",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_iter = iter(train_ds)\n",
    "inputs, labels = next(dataset_iter) \n",
    "#Are there two assignments happening on every iteration because dataset stores inputs with labels?\n",
    "# print(inputs.shape) #(100, 32, 32, 12)\n",
    "# print(labels.shape) #(100, 32, 32, 1)\n",
    "# print(inputs[0, :, :, 11]) #Trying to grab the previous fire mask. (Apparent) success!\n",
    "# print(labels[0,:, :, 0]) #Ok, I think the labels are the fire mask. (That also accords with standard usage of the term.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34eaf288",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-19T19:43:16.791082Z",
     "iopub.status.busy": "2022-03-19T19:43:16.790340Z",
     "iopub.status.idle": "2022-03-19T19:43:16.792088Z",
     "shell.execute_reply": "2022-03-19T19:43:16.792554Z",
     "shell.execute_reply.started": "2022-03-19T18:27:11.384462Z"
    },
    "id": "bnG0_l_ChjUt",
    "papermill": {
     "duration": 0.038443,
     "end_time": "2022-03-19T19:43:16.792727",
     "exception": false,
     "start_time": "2022-03-19T19:43:16.754284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TITLES = [\n",
    "  'Elevation',\n",
    "  'Wind\\ndirection',\n",
    "  'Wind\\nvelocity',\n",
    "  'Min\\ntemp',\n",
    "  'Max\\ntemp',\n",
    "  'Humidity',\n",
    "  'Precip',\n",
    "  'Drought',\n",
    "  'Vegetation',\n",
    "  'Population\\ndensity',\n",
    "  'Energy\\nrelease\\ncomponent',\n",
    "  'Previous\\nfire\\nmask',\n",
    "  'Fire\\nmask'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b1d9be",
   "metadata": {
    "papermill": {
     "duration": 0.030121,
     "end_time": "2022-03-19T19:43:16.853562",
     "exception": false,
     "start_time": "2022-03-19T19:43:16.823441",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Define some helper variables for the plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f29841e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-19T19:43:16.917983Z",
     "iopub.status.busy": "2022-03-19T19:43:16.917276Z",
     "iopub.status.idle": "2022-03-19T19:43:16.921791Z",
     "shell.execute_reply": "2022-03-19T19:43:16.922336Z",
     "shell.execute_reply.started": "2022-03-19T18:27:11.396068Z"
    },
    "id": "G0E6lWR9beD0",
    "papermill": {
     "duration": 0.038215,
     "end_time": "2022-03-19T19:43:16.922540",
     "exception": false,
     "start_time": "2022-03-19T19:43:16.884325",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of rows of data samples to plot\n",
    "n_rows = 5\n",
    "# Number of data variables\n",
    "n_features = inputs.shape[3]\n",
    "# Variables for controllong the color map for the fire masks\n",
    "CMAP = colors.ListedColormap(['black', 'silver', 'orangered'])\n",
    "BOUNDS = [-1, -0.1, 0.001, 1]\n",
    "NORM = colors.BoundaryNorm(BOUNDS, CMAP.N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9f188a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-19T19:43:16.988511Z",
     "iopub.status.busy": "2022-03-19T19:43:16.987727Z",
     "iopub.status.idle": "2022-03-19T19:43:20.389082Z",
     "shell.execute_reply": "2022-03-19T19:43:20.389613Z",
     "shell.execute_reply.started": "2022-03-19T18:27:11.407179Z"
    },
    "id": "sPtKQzQv71J_",
    "outputId": "6694ad6e-0044-4fbc-b184-f615ac14a885",
    "papermill": {
     "duration": 3.436096,
     "end_time": "2022-03-19T19:43:20.389787",
     "exception": false,
     "start_time": "2022-03-19T19:43:16.953691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,6.5))\n",
    "\n",
    "for i in range(n_rows):\n",
    "  for j in range(n_features + 1):\n",
    "    plt.subplot(n_rows, n_features + 1, i * (n_features + 1) + j + 1)\n",
    "    if i == 0:\n",
    "      plt.title(TITLES[j], fontsize=13)\n",
    "    if j < n_features - 1:\n",
    "      plt.imshow(inputs[i, :, :, j], cmap='viridis')\n",
    "    if j == n_features - 1:\n",
    "      plt.imshow(inputs[i, :, :, -1], cmap=CMAP, norm=NORM)\n",
    "    if j == n_features:\n",
    "      plt.imshow(labels[i, :, :, 0], cmap=CMAP, norm=NORM) \n",
    "    plt.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bed82a",
   "metadata": {
    "id": "bzCgCoxtgP-f",
    "papermill": {
     "duration": 0.0562,
     "end_time": "2022-03-19T19:43:16.660951",
     "exception": false,
     "start_time": "2022-03-19T19:43:16.604751",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Get metrics on all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef864c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def tf_welford(ds, cnt_limit=-1):\n",
    "    ds_numpy = tfds.as_numpy(ds)\n",
    "    w_min = np.full(12, float('inf'), dtype=np.float64)\n",
    "    w_max = np.full(12, float('-inf'), dtype=np.float64)\n",
    "    w_mean = np.zeros(12, dtype=np.float64)\n",
    "    w_stdev = np.zeros(12, dtype=np.float64)\n",
    "    sumsq = np.zeros(12, dtype=np.float64)\n",
    "    \n",
    "\n",
    "    cnt = 0.0\n",
    "    for da in tqdm(ds_numpy):\n",
    "        da = da[0]\n",
    "        for i in range(da.shape[0]):\n",
    "            cnt += 1.0\n",
    "            sample = da[i]\n",
    "            img_size = sample.shape[0]\n",
    "            for j in range(12):\n",
    "                x = sample[: , :,j]\n",
    "                x = tf.reshape(x, [-1])\n",
    "                \n",
    "                w_min[j] = min(w_min[j], tf.reduce_min(x))\n",
    "                w_max[j] = max(w_max[j], tf.reduce_max(x))\n",
    "                \n",
    "                delta = tf.math.reduce_mean(x - w_mean[j]).numpy()\n",
    "                w_mean[j] += delta / cnt\n",
    "                # variance calculation deviates a little from Welford as it uses a batch of 4096 \n",
    "                sumsq[j] += tf.math.reduce_sum(tf.math.multiply(x, x)).numpy()\n",
    "                w_stdev[j] = np.sqrt((sumsq[j]/(cnt*(img_size*img_size))) - w_mean[j]*w_mean[j])\n",
    "\n",
    "            if cnt == float(cnt_limit):\n",
    "                break \n",
    "        else:\n",
    "            continue\n",
    "        break\n",
    "\n",
    "    return w_min, w_max, w_mean, w_stdev\n",
    "\n",
    "\n",
    "CHK_COUNT = 25000\n",
    "\n",
    "w_min, w_max, w_mean, w_stdev = tf_welford(dataset, CHK_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6cb1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(12):\n",
    "    print(f'Feature {i+1}:')\n",
    "    print(f'Min: {w_min[i]}')# Plotting function\n",
    "    print(f'Max: {w_max[i]}')\n",
    "    print(f'Mean: {w_mean[i]}')\n",
    "    print(f'Standard Deviation: {w_stdev[i]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b1c356",
   "metadata": {
    "id": "bzCgCoxtgP-f",
    "papermill": {
     "duration": 0.0562,
     "end_time": "2022-03-19T19:43:16.660951",
     "exception": false,
     "start_time": "2022-03-19T19:43:16.604751",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train U-net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e020c7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, BatchNormalization, Conv2DTranspose\n",
    "from keras import Input\n",
    "def EncoderMiniBlock(inputs, n_filters=32, dropout_prob=0.3, max_pooling=True):\n",
    "    conv = Conv2D(n_filters, \n",
    "                  3,  # filter size\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  kernel_initializer='HeNormal')(inputs)\n",
    "\n",
    "    conv = Conv2D(n_filters, \n",
    "                  3,  # filter size\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  kernel_initializer='HeNormal')(conv)\n",
    "  \n",
    "    conv = BatchNormalization()(conv, training=False)\n",
    "    if dropout_prob > 0:     \n",
    "        conv = tf.keras.layers.Dropout(dropout_prob)(conv)\n",
    "    if max_pooling:\n",
    "        next_layer = tf.keras.layers.MaxPooling2D(pool_size = (2,2))(conv)    \n",
    "    else:\n",
    "        next_layer = conv\n",
    "    skip_connection = conv    \n",
    "    return next_layer, skip_connection\n",
    "\n",
    "def DecoderMiniBlock(prev_layer_input, skip_layer_input, n_filters=32):\n",
    "    up = Conv2DTranspose(\n",
    "                 n_filters,\n",
    "                 (3,3),\n",
    "                 strides=(2,2),\n",
    "                 padding='same')(prev_layer_input)\n",
    "    merge = tf.concat([up, skip_layer_input], axis=3)\n",
    "    conv = Conv2D(n_filters, \n",
    "                 3,  \n",
    "                 activation='relu',\n",
    "                 padding='same',\n",
    "                 kernel_initializer='HeNormal')(merge)\n",
    "    conv = Conv2D(n_filters,\n",
    "                 3, \n",
    "                 activation='relu',\n",
    "                 padding='same',\n",
    "                 kernel_initializer='HeNormal')(conv)\n",
    "    return conv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90ebf48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def UNetCompiled(input_size=(64, 64, 12), n_filters=32, n_classes=2):\n",
    "    \"\"\"\n",
    "    Combine both encoder and decoder blocks according to the U-Net research paper\n",
    "    Return the model as output \n",
    "    \"\"\"\n",
    "    # Input size represent the size of 1 image (the size used for pre-processing) \n",
    "    inputs = Input(input_size)\n",
    "    \n",
    "    # Encoder includes multiple convolutional mini blocks with different maxpooling, dropout and filter parameters\n",
    "    # Observe that the filters are increasing as we go deeper into the network which will increasse the # channels of the image \n",
    "    cblock1 = EncoderMiniBlock(inputs, n_filters,dropout_prob=0, max_pooling=True)\n",
    "    cblock2 = EncoderMiniBlock(cblock1[0],n_filters*2,dropout_prob=0, max_pooling=True)\n",
    "    cblock3 = EncoderMiniBlock(cblock2[0], n_filters*4,dropout_prob=0, max_pooling=True)\n",
    "    cblock4 = EncoderMiniBlock(cblock3[0], n_filters*8,dropout_prob=0.3, max_pooling=True)\n",
    "    cblock5 = EncoderMiniBlock(cblock4[0], n_filters*16, dropout_prob=0.3, max_pooling=False) \n",
    "    \n",
    "    # Decoder includes multiple mini blocks with decreasing number of filters\n",
    "    # Observe the skip connections from the encoder are given as input to the decoder\n",
    "    # Recall the 2nd output of encoder block was skip connection, hence cblockn[1] is used\n",
    "    ublock6 = DecoderMiniBlock(cblock5[0], cblock4[1],  n_filters * 8)\n",
    "    ublock7 = DecoderMiniBlock(ublock6, cblock3[1],  n_filters * 4)\n",
    "    ublock8 = DecoderMiniBlock(ublock7, cblock2[1],  n_filters * 2)\n",
    "    ublock9 = DecoderMiniBlock(ublock8, cblock1[1],  n_filters)\n",
    "\n",
    "    # Complete the model with 1 3x3 convolution layer (Same as the prev Conv Layers)\n",
    "    # Followed by a 1x1 Conv layer to get the image to the desired size. \n",
    "    # Observe the number of channels will be equal to number of output classes\n",
    "    conv9 = Conv2D(n_filters,\n",
    "                 3,\n",
    "                 activation='relu',\n",
    "                 padding='same',\n",
    "                 kernel_initializer='he_normal')(ublock9)\n",
    "\n",
    "    conv10 = Conv2D(n_classes, 1, padding='same')(conv9)\n",
    "    \n",
    "    # Define the model\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=conv10)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a152ae65",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.profiler.experimental.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f37929dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ca6f1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fea78815",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_callbacks = []\n",
    "\n",
    "list_of_callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                min_delta=0, \n",
    "                                patience=0, \n",
    "                                verbose=0, \n",
    "                                mode='auto', \n",
    "                                baseline=None, \n",
    "                                restore_best_weights=False))\n",
    "\n",
    "list_of_callbacks.append(tf.keras.callbacks.ModelCheckpoint('./models/', \n",
    "                                     monitor='val_loss', \n",
    "                                     verbose=0, \n",
    "                                     save_best_only=False,\n",
    "                                     save_weights_only=False, \n",
    "                                     mode='auto', \n",
    "                                     save_freq='epoch'))\n",
    "\n",
    "\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "list_of_callbacks.append(tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, profile_batch='10, 15'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "966f392b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "      9/Unknown - 7s 561ms/step - loss: 0.5720 - accuracy: 0.7698ERROR:tensorflow:Failed to start profiler: Another profiler is running.\n",
      "     98/Unknown - 52s 512ms/step - loss: 0.4262 - accuracy: 0.8358INFO:tensorflow:Assets written to: ./models\\assets\n",
      "98/98 [==============================] - 64s 628ms/step - loss: 0.4262 - accuracy: 0.8358 - val_loss: 0.2871 - val_accuracy: 0.9058\n",
      "Epoch 2/2\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4060 - accuracy: 0.8371INFO:tensorflow:Assets written to: ./models\\assets\n",
      "98/98 [==============================] - 41s 413ms/step - loss: 0.4060 - accuracy: 0.8371 - val_loss: 0.2846 - val_accuracy: 0.9070\n",
      "Took 272.17232760000024 to train\n"
     ]
    }
   ],
   "source": [
    "tic = timeit.default_timer()\n",
    "\n",
    "tf.profiler.experimental.start(log_dir)\n",
    "unet = UNetCompiled()\n",
    "unet.compile(optimizer=tf.keras.optimizers.Adam(beta_1=0.99), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "results = unet.fit(train_ds, validation_data=valid_ds, epochs=100,callbacks=list_of_callbacks)\n",
    "tf.profiler.experimental.stop()\n",
    "K.clear_session()\n",
    "\n",
    "toc = timeit.default_timer()\n",
    "print(f\"Took {toc - tic} to train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9acb655c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 35644), started 1 day, 19:36:05 ago. (Use '!kill 35644' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-c2793ab2c9e901e1\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-c2793ab2c9e901e1\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=C:/Users/kamen/OneDrive/Documents/shared_drive/Thesis/Wildfire_Data_Generator/logs/fit  --port=6006  --host localhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "97908b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "if not device_name:\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bee5b777",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iter = iter(test_ds)\n",
    "inputs, labels = next(test_iter) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1267e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = unet.predict(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d1460a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = tf.math.argmax(pred, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e25566be",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelss = tf.reshape(labels, [100,64,64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb4e6a73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77605224"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = tf.keras.metrics.Accuracy()\n",
    "\n",
    "m.update_state(preds, labelss)\n",
    "m.result().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c221995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74909115"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = tf.keras.metrics.AUC(num_thresholds=3)\n",
    "m.update_state(preds, labelss)\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6ff5e420",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x28bcf333d00>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(inputs[20, :, :, -1], cmap=CMAP, norm=NORM) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "713d6616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x28ddc0569d0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(labelss[20, :, :], cmap=CMAP, norm=NORM) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6cdbd19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x28de3c5c1c0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(preds[20, :, :], cmap=CMAP, norm=NORM) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34db266",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e5bc95e98aae8e8660e5c1aff5e536338d0a5ad53799cc4cc0c593ca43767c4d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 27.717773,
   "end_time": "2022-03-19T19:43:23.629414",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-03-19T19:42:55.911641",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
